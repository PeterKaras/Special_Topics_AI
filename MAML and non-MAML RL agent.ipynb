{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61496b17-2891-458b-af34-d6f411ce733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from gymnasium.core import Env\n",
    "from collections import deque, OrderedDict\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Optional, Callable, List, Generator, Tuple, Literal, overload, Any\n",
    "import math\n",
    "#from env_sets import EnvSet, possible_envs\n",
    "from scipy.stats import norm, uniform\n",
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db183cc2-5493-4847-a72b-8c43c20bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeEnv(gym.Env):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "        # Running mean and variance for normalization\n",
    "        self.obs_mean = np.zeros(self.observation_space.shape)\n",
    "        self.obs_var = np.ones(self.observation_space.shape)\n",
    "        self.epsilon = 1e-8\n",
    "        self.clip_obs = 10.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update_obs_stats(self, obs):\n",
    "        # Increment count\n",
    "        self.count += 1\n",
    "        # Update mean and variance\n",
    "        #print(obs[0].shape, self.obs_mean.shape)\n",
    "        delta = obs - self.obs_mean\n",
    "        self.obs_mean += delta / self.count\n",
    "        delta2 = obs - self.obs_mean\n",
    "        self.obs_var += delta * delta2\n",
    "\n",
    "    def normalize_obs(self, obs):\n",
    "        return np.clip((obs - self.obs_mean) / (np.sqrt(self.obs_var / self.count) + self.epsilon), -self.clip_obs, self.clip_obs)\n",
    "\n",
    "    def denormalize_obs(self, obs):\n",
    "        return obs * (np.sqrt(self.obs_var / self.count) + self.epsilon) + self.obs_mean\n",
    "\n",
    "    def get_normalized_stats(self):\n",
    "        return self.obs_mean, np.sqrt(self.obs_var / self.count)\n",
    "\n",
    "    def set_normalized_stats(self, obs_mean, obs_std):\n",
    "        self.obs_mean = obs_mean\n",
    "        self.obs_var = obs_std ** 2\n",
    "        self.count = 1  # Reset count to 1 since we are setting stats manually\n",
    "\n",
    "    def step(self, action):\n",
    "        truncated  =False\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        \n",
    "        self.update_obs_stats(obs)\n",
    "        obs_normalized = self.normalize_obs(obs)\n",
    "        return obs_normalized, reward, done, truncated, info\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()[0]\n",
    "        self.update_obs_stats(obs)\n",
    "        obs_normalized = self.normalize_obs(obs)\n",
    "        return obs_normalized\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode=mode)\n",
    "\n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return state, action, reward, next_state, done\n",
    "        #return (np.array(states), np.array(actions), np.array(rewards),np.array(next_states), np.array(dones))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class ContinuousMountainCarCustom(gym.Env):\n",
    "    def __init__(self, gravity, mountain_height, slope):\n",
    "        self.env = gym.make('MountainCar-v0')  #, render_mode=\"human\")\n",
    "        self.env.gravity = gravity\n",
    "        self.env.height = mountain_height\n",
    "        self.env.slope = slope\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b4971a0-8a6d-4bdc-9c77-9b17eb9b33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tasks(gravity_range, height_range, slope_range, num_samples=10):\n",
    "    gravity = np.random.uniform(*gravity_range, size=num_samples)\n",
    "    height = np.random.uniform(*height_range, size=num_samples)\n",
    "    slope = np.random.uniform(*slope_range, size=num_samples)\n",
    "    return list(zip(gravity, height, slope))\n",
    "\n",
    "# Define parameter ranges\n",
    "gravity_range = [0.002, 0.003]\n",
    "height_range = [0.35, 0.55]\n",
    "slope_range = [2.0, 4.0]\n",
    "\n",
    "# Create task pool\n",
    "task_pool = generate_tasks(gravity_range, height_range, slope_range, num_samples=10)\n",
    "random.shuffle(task_pool)\n",
    "\n",
    "# Train and test splits\n",
    "train_tasks = task_pool[:7]\n",
    "test_tasks = task_pool[7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c83d6-1756-428e-a84b-4dd286e26606",
   "metadata": {},
   "source": [
    "## MAML for Mountain car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a8f1602-e7bc-4f12-82f5-b3ca235ba089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maml_train(meta_model, tasks, outer_steps=25, inner_steps=10, lr_inner=0.01, lr_outer=0.001, gamma=0.99, capacity = 1000, max_steps=500):\n",
    "    meta_optimizer = optim.Adam(meta_model.parameters(), lr=lr_outer)\n",
    "    task_models = []\n",
    "    \n",
    "    for outer_step in range(outer_steps):\n",
    "        meta_gradients = None\n",
    "\n",
    "        for task_idx, task_params in enumerate(tasks):\n",
    "            gravity, height, slope = task_params\n",
    "            env = ContinuousMountainCarCustom(gravity, height, slope)\n",
    "            env = NormalizeEnv(env)\n",
    "            model = deepcopy(meta_model)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr_inner)\n",
    "\n",
    "            # Inner loop\n",
    "            buffer = ReplayBuffer(capacity=10000)\n",
    "            \n",
    "            for _ in range(inner_steps):\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                for _ in range(max_steps):\n",
    "                    #print(state) #, torch.FloatTensor(state), torch.FloatTensor(state).detach())\n",
    "                    #action = model(torch.FloatTensor(state)).detach().numpy()      # for continuous action space\n",
    "                    action_values = model(torch.FloatTensor(state)).detach().numpy()\n",
    "                    action = np.argmax(action_values)\n",
    "                    #print(action)\n",
    "                    next_state, reward, done, truncated, _ = env.step(action)\n",
    "                    buffer.add(state, action, reward, next_state, done)\n",
    "                    state = next_state\n",
    "\n",
    "                print(f\"Testing on task {task_idx + 1}/{len(test_tasks)}: {task_params}\")\n",
    "                # we can update \n",
    "                #task_rewards.append(sum(rewards))\n",
    "                # Update model with sampled experiences\n",
    "                states, actions, rewards, next_states, dones = buffer.sample(batch_size=32)\n",
    "                states, actions, rewards, next_states, dones = map(torch.FloatTensor, (states, actions, rewards, next_states, dones))\n",
    "                q_values = model(states).gather(1, actions.long().unsqueeze(1)).squeeze(1)\n",
    "                next_q_values = model(next_states).max(1)[0]\n",
    "                target_q_values = rewards + (1 - dones) * gamma * next_q_values\n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                # for applying DDQN like meta model we have to make one target_meta model as well\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Compute gradients for the meta-model\n",
    "            task_gradients = torch.autograd.grad(loss, meta_model.parameters(), retain_graph=True)\n",
    "            if meta_gradients is None:\n",
    "                meta_gradients = task_gradients\n",
    "            else:\n",
    "                meta_gradients = [g + tg for g, tg in zip(meta_gradients, task_gradients)]\n",
    "\n",
    "        # Outer loop update\n",
    "        meta_optimizer.zero_grad()\n",
    "        for param, grad in zip(meta_model.parameters(), meta_gradients):\n",
    "            param.grad = grad / len(tasks)\n",
    "        meta_optimizer.step()\n",
    "\n",
    "        print(f\"Outer step {outer_step + 1}/{outer_steps} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2e5499-f2a0-4440-88f9-71427308f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maml_test(meta_model, tasks, num_episodes=10):\n",
    "    rewards = []\n",
    "\n",
    "    for task_params in tasks:\n",
    "        gravity, height, slope = task_params\n",
    "        env = ContinuousMountainCarCustom(gravity, height, slope)\n",
    "        env = NormalizeEnv(env)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    #action = meta_model(torch.FloatTensor(state)).numpy()     # for cont action space\n",
    "                    action_values = model(torch.FloatTensor(state)).detach().numpy()\n",
    "                    action = np.argmax(action_values)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward / num_episodes)\n",
    "\n",
    "    return np.mean(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a10acb-407f-4e3e-a623-a03aaed2bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Env = gym.make('MountainCar-v0')\n",
    "\n",
    "state_dim = Env.observation_space.shape[0]  # MountainCar state space\n",
    "action_dim = Env.action_space.n  \n",
    "meta_model = DQN(state_dim, action_dim)\n",
    "\n",
    "# Train and test\n",
    "maml_train(meta_model, train_tasks)\n",
    "meta_reward = maml_test(meta_model, test_tasks)\n",
    "\n",
    "print(f\"Meta-Learning Test Reward: {meta_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63155f36-5237-4e01-9780-339c8046b5eb",
   "metadata": {},
   "source": [
    "## Non-MAML training of agent for mountain car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a331521-a0dd-422f-9a5d-0ce4ee22e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "Episode 20, Task 4/7, Reward: -500.0\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "Episode 40, Task 1/7, Reward: -500.0\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "Episode 60, Task 4/7, Reward: -500.0\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "Episode 80, Task 2/7, Reward: -500.0\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n",
      "50000\n",
      "Episode 100, Task 1/7, Reward: -500.0\n",
      "50500\n",
      "51000\n",
      "51500\n",
      "52000\n",
      "52500\n",
      "53000\n",
      "53500\n",
      "54000\n",
      "54500\n",
      "55000\n",
      "55500\n",
      "56000\n",
      "56500\n",
      "57000\n",
      "57500\n",
      "58000\n",
      "58500\n",
      "59000\n",
      "59500\n",
      "60000\n",
      "Episode 120, Task 2/7, Reward: -500.0\n",
      "60500\n",
      "61000\n",
      "61500\n",
      "62000\n",
      "62500\n",
      "63000\n",
      "63500\n",
      "64000\n",
      "64500\n",
      "65000\n",
      "65500\n",
      "66000\n",
      "66500\n",
      "67000\n",
      "67500\n",
      "68000\n",
      "68500\n",
      "69000\n",
      "69500\n",
      "70000\n",
      "Episode 140, Task 3/7, Reward: -500.0\n",
      "70500\n",
      "71000\n",
      "71500\n",
      "72000\n",
      "72500\n",
      "73000\n",
      "73500\n",
      "74000\n",
      "74500\n",
      "75000\n",
      "75500\n",
      "76000\n",
      "76500\n",
      "77000\n",
      "77500\n",
      "78000\n",
      "78500\n",
      "79000\n",
      "79500\n",
      "80000\n",
      "Episode 160, Task 3/7, Reward: -500.0\n",
      "80500\n",
      "81000\n",
      "81500\n",
      "82000\n",
      "82500\n",
      "83000\n",
      "83500\n",
      "84000\n",
      "84500\n",
      "85000\n",
      "85500\n",
      "86000\n",
      "86500\n",
      "87000\n",
      "87500\n",
      "88000\n",
      "88500\n",
      "89000\n",
      "89500\n",
      "90000\n",
      "Episode 180, Task 7/7, Reward: -500.0\n",
      "90500\n",
      "91000\n",
      "91500\n",
      "92000\n",
      "92500\n",
      "93000\n",
      "93500\n",
      "94000\n",
      "94500\n",
      "95000\n",
      "95500\n",
      "96000\n",
      "96500\n",
      "97000\n",
      "97500\n",
      "98000\n",
      "98500\n",
      "99000\n",
      "99500\n",
      "100000\n",
      "Episode 200, Task 4/7, Reward: -500.0\n",
      "100500\n",
      "101000\n",
      "101500\n",
      "102000\n",
      "102500\n",
      "103000\n",
      "103500\n",
      "104000\n",
      "104500\n",
      "105000\n",
      "105500\n",
      "106000\n",
      "106500\n",
      "107000\n",
      "107500\n",
      "108000\n",
      "108500\n",
      "109000\n",
      "109500\n",
      "110000\n",
      "Episode 220, Task 7/7, Reward: -500.0\n",
      "110500\n",
      "111000\n",
      "111500\n",
      "112000\n",
      "112500\n",
      "113000\n",
      "113500\n",
      "114000\n",
      "114500\n",
      "115000\n",
      "115500\n",
      "116000\n",
      "116500\n",
      "117000\n",
      "117500\n",
      "118000\n",
      "118500\n",
      "119000\n",
      "119500\n",
      "120000\n",
      "Episode 240, Task 1/7, Reward: -500.0\n",
      "120500\n",
      "121000\n",
      "121500\n",
      "122000\n",
      "122500\n",
      "123000\n",
      "123500\n",
      "124000\n",
      "124500\n",
      "125000\n",
      "125500\n",
      "126000\n",
      "126500\n",
      "127000\n",
      "127500\n",
      "128000\n",
      "128500\n",
      "129000\n",
      "129500\n",
      "130000\n",
      "Episode 260, Task 7/7, Reward: -500.0\n",
      "130500\n",
      "131000\n",
      "131500\n",
      "132000\n",
      "132500\n",
      "133000\n",
      "133500\n",
      "134000\n",
      "134500\n",
      "135000\n",
      "135500\n",
      "136000\n",
      "136500\n",
      "137000\n",
      "137500\n",
      "138000\n",
      "138500\n",
      "139000\n",
      "139500\n",
      "140000\n",
      "Episode 280, Task 6/7, Reward: -500.0\n",
      "140500\n",
      "141000\n",
      "141500\n",
      "142000\n",
      "142500\n",
      "143000\n",
      "143500\n",
      "144000\n",
      "144500\n",
      "145000\n",
      "145500\n",
      "146000\n",
      "146500\n",
      "147000\n",
      "147500\n",
      "148000\n",
      "148500\n",
      "149000\n",
      "149500\n",
      "150000\n",
      "Episode 300, Task 4/7, Reward: -500.0\n",
      "150500\n",
      "151000\n",
      "151500\n",
      "152000\n",
      "152500\n",
      "153000\n",
      "153500\n",
      "154000\n",
      "154500\n",
      "155000\n",
      "155500\n",
      "156000\n",
      "156500\n",
      "157000\n",
      "157500\n",
      "158000\n",
      "158500\n",
      "159000\n",
      "159500\n",
      "160000\n",
      "Episode 320, Task 6/7, Reward: -500.0\n",
      "160500\n",
      "161000\n",
      "161500\n",
      "162000\n",
      "162500\n",
      "163000\n",
      "163500\n",
      "164000\n",
      "164500\n",
      "165000\n",
      "165500\n",
      "166000\n",
      "166500\n",
      "167000\n",
      "167500\n",
      "168000\n",
      "168500\n",
      "169000\n",
      "169500\n",
      "170000\n",
      "Episode 340, Task 2/7, Reward: -500.0\n",
      "170500\n",
      "171000\n",
      "171500\n",
      "172000\n",
      "172500\n",
      "173000\n",
      "173500\n",
      "174000\n",
      "174500\n",
      "175000\n",
      "175500\n",
      "176000\n",
      "176500\n",
      "177000\n",
      "177500\n",
      "178000\n",
      "178500\n",
      "179000\n",
      "179500\n",
      "180000\n",
      "Episode 360, Task 5/7, Reward: -500.0\n",
      "180500\n",
      "181000\n",
      "181500\n",
      "182000\n",
      "182500\n",
      "183000\n",
      "183500\n",
      "184000\n",
      "184500\n",
      "185000\n",
      "185500\n",
      "186000\n",
      "186500\n",
      "187000\n",
      "187500\n",
      "188000\n",
      "188500\n",
      "189000\n",
      "189500\n",
      "190000\n",
      "Episode 380, Task 7/7, Reward: -500.0\n",
      "190500\n",
      "191000\n",
      "191500\n",
      "192000\n",
      "192500\n",
      "193000\n",
      "193500\n",
      "194000\n",
      "194500\n",
      "195000\n",
      "195500\n",
      "196000\n",
      "196500\n",
      "197000\n",
      "197500\n",
      "198000\n",
      "198500\n",
      "199000\n",
      "199500\n",
      "200000\n",
      "Episode 400, Task 2/7, Reward: -500.0\n",
      "200500\n",
      "201000\n",
      "201500\n",
      "202000\n",
      "202500\n",
      "203000\n",
      "203500\n",
      "204000\n",
      "204500\n",
      "205000\n",
      "205500\n",
      "206000\n",
      "206500\n",
      "207000\n",
      "207500\n",
      "208000\n",
      "208500\n",
      "209000\n",
      "209500\n",
      "210000\n",
      "Episode 420, Task 1/7, Reward: -500.0\n",
      "210500\n",
      "211000\n",
      "211500\n",
      "212000\n",
      "212500\n",
      "213000\n",
      "213500\n",
      "214000\n",
      "214500\n",
      "215000\n",
      "215500\n",
      "216000\n",
      "216500\n",
      "217000\n",
      "217500\n",
      "218000\n",
      "218500\n",
      "219000\n",
      "219500\n",
      "220000\n",
      "Episode 440, Task 6/7, Reward: -500.0\n",
      "220500\n",
      "221000\n",
      "221500\n",
      "222000\n",
      "222500\n",
      "223000\n",
      "223500\n",
      "224000\n",
      "224500\n",
      "225000\n",
      "225500\n",
      "226000\n",
      "226500\n",
      "227000\n",
      "227500\n",
      "228000\n",
      "228500\n",
      "229000\n",
      "229500\n",
      "230000\n",
      "Episode 460, Task 3/7, Reward: -500.0\n",
      "230500\n",
      "231000\n",
      "231500\n",
      "232000\n",
      "232500\n",
      "233000\n",
      "233500\n",
      "234000\n",
      "234500\n",
      "235000\n",
      "235500\n",
      "236000\n",
      "236500\n",
      "237000\n",
      "237500\n",
      "238000\n",
      "238500\n",
      "239000\n",
      "239500\n",
      "240000\n",
      "Episode 480, Task 3/7, Reward: -500.0\n",
      "240500\n",
      "241000\n",
      "241500\n",
      "242000\n",
      "242500\n",
      "243000\n",
      "243500\n",
      "244000\n",
      "244500\n",
      "245000\n",
      "245500\n",
      "246000\n",
      "246500\n",
      "247000\n",
      "247500\n",
      "248000\n",
      "248500\n",
      "249000\n",
      "249500\n",
      "250000\n",
      "Episode 500, Task 7/7, Reward: -500.0\n",
      "250500\n",
      "251000\n",
      "251500\n",
      "252000\n",
      "252500\n",
      "253000\n",
      "253500\n",
      "254000\n",
      "254500\n",
      "255000\n",
      "255500\n",
      "256000\n",
      "256500\n",
      "257000\n",
      "257500\n",
      "258000\n",
      "258500\n",
      "259000\n",
      "259500\n",
      "260000\n",
      "Episode 520, Task 2/7, Reward: -500.0\n",
      "260500\n",
      "261000\n",
      "261500\n",
      "262000\n",
      "262500\n",
      "263000\n",
      "263500\n",
      "264000\n",
      "264500\n",
      "265000\n",
      "265500\n",
      "266000\n",
      "266500\n",
      "267000\n",
      "267500\n",
      "268000\n",
      "268500\n",
      "269000\n",
      "269500\n",
      "270000\n",
      "Episode 540, Task 3/7, Reward: -500.0\n",
      "270500\n",
      "271000\n",
      "271500\n",
      "272000\n",
      "272500\n",
      "273000\n",
      "273500\n",
      "274000\n",
      "274500\n",
      "275000\n",
      "275500\n",
      "276000\n",
      "276500\n",
      "277000\n",
      "277500\n",
      "278000\n",
      "278500\n",
      "279000\n",
      "279500\n",
      "280000\n",
      "Episode 560, Task 5/7, Reward: -500.0\n",
      "280500\n",
      "281000\n",
      "281500\n",
      "282000\n",
      "282500\n",
      "283000\n",
      "283500\n",
      "284000\n",
      "284500\n",
      "285000\n",
      "285500\n",
      "286000\n",
      "286500\n",
      "287000\n",
      "287500\n",
      "288000\n",
      "288500\n",
      "289000\n",
      "289500\n",
      "290000\n",
      "Episode 580, Task 6/7, Reward: -500.0\n",
      "290500\n",
      "291000\n",
      "291500\n",
      "292000\n",
      "292500\n",
      "293000\n",
      "293500\n",
      "294000\n",
      "294500\n",
      "295000\n",
      "295500\n",
      "296000\n",
      "296500\n",
      "297000\n",
      "297500\n",
      "298000\n",
      "298500\n",
      "299000\n",
      "299500\n",
      "300000\n",
      "Episode 600, Task 4/7, Reward: -500.0\n",
      "300500\n",
      "301000\n",
      "301500\n",
      "302000\n",
      "302500\n",
      "303000\n",
      "303500\n",
      "304000\n",
      "304500\n",
      "305000\n",
      "305500\n",
      "306000\n",
      "306500\n",
      "307000\n",
      "307500\n",
      "308000\n",
      "308500\n",
      "309000\n",
      "309500\n",
      "310000\n",
      "Episode 620, Task 1/7, Reward: -500.0\n",
      "310500\n",
      "311000\n",
      "311500\n",
      "312000\n",
      "312500\n",
      "313000\n",
      "313500\n",
      "314000\n",
      "314500\n",
      "315000\n",
      "315500\n",
      "316000\n",
      "316500\n",
      "317000\n",
      "317500\n",
      "318000\n",
      "318500\n",
      "319000\n",
      "319500\n",
      "320000\n",
      "Episode 640, Task 6/7, Reward: -500.0\n",
      "320500\n",
      "321000\n",
      "321500\n",
      "322000\n",
      "322500\n",
      "323000\n",
      "323500\n",
      "324000\n",
      "324500\n",
      "325000\n",
      "325500\n",
      "326000\n",
      "326500\n",
      "327000\n",
      "327500\n",
      "328000\n",
      "328500\n",
      "329000\n",
      "329500\n",
      "330000\n",
      "Episode 660, Task 7/7, Reward: -500.0\n",
      "330500\n",
      "331000\n",
      "331500\n",
      "332000\n",
      "332500\n",
      "333000\n",
      "333500\n",
      "334000\n",
      "334500\n",
      "335000\n",
      "335500\n",
      "336000\n",
      "336500\n",
      "337000\n",
      "337500\n",
      "338000\n",
      "338500\n",
      "339000\n",
      "339500\n",
      "340000\n",
      "Episode 680, Task 6/7, Reward: -500.0\n",
      "340500\n",
      "341000\n",
      "341500\n",
      "342000\n",
      "342500\n",
      "343000\n",
      "343500\n",
      "344000\n",
      "344500\n",
      "345000\n",
      "345500\n",
      "346000\n",
      "346500\n",
      "347000\n",
      "347500\n",
      "348000\n",
      "348500\n",
      "349000\n",
      "349500\n",
      "350000\n",
      "Episode 700, Task 4/7, Reward: -500.0\n",
      "350500\n",
      "351000\n",
      "351500\n",
      "352000\n",
      "352500\n",
      "353000\n",
      "353500\n",
      "354000\n",
      "354500\n",
      "355000\n",
      "355500\n",
      "356000\n",
      "356500\n",
      "357000\n",
      "357500\n",
      "358000\n",
      "358500\n",
      "359000\n",
      "359500\n",
      "360000\n",
      "Episode 720, Task 6/7, Reward: -500.0\n",
      "360500\n",
      "361000\n",
      "361500\n",
      "362000\n",
      "362500\n",
      "363000\n",
      "363500\n",
      "364000\n",
      "364500\n",
      "365000\n",
      "365500\n",
      "366000\n",
      "366500\n",
      "367000\n",
      "367500\n",
      "368000\n",
      "368500\n",
      "369000\n",
      "369500\n",
      "370000\n",
      "Episode 740, Task 5/7, Reward: -500.0\n",
      "370500\n",
      "371000\n",
      "371500\n",
      "372000\n",
      "372500\n",
      "373000\n",
      "373500\n",
      "374000\n",
      "374500\n",
      "375000\n",
      "375500\n",
      "376000\n",
      "376500\n",
      "377000\n",
      "377500\n",
      "378000\n",
      "378500\n",
      "379000\n",
      "379500\n",
      "380000\n",
      "Episode 760, Task 6/7, Reward: -500.0\n",
      "380500\n",
      "381000\n",
      "381500\n",
      "382000\n",
      "382500\n",
      "383000\n",
      "383500\n",
      "384000\n",
      "384500\n",
      "385000\n",
      "385500\n",
      "386000\n",
      "386500\n",
      "387000\n",
      "387500\n",
      "388000\n",
      "388500\n",
      "389000\n",
      "389500\n",
      "390000\n",
      "Episode 780, Task 5/7, Reward: -500.0\n",
      "390500\n",
      "391000\n",
      "391500\n",
      "392000\n",
      "392500\n",
      "393000\n",
      "393500\n",
      "394000\n",
      "394500\n",
      "395000\n",
      "395500\n",
      "396000\n",
      "396500\n",
      "397000\n",
      "397500\n",
      "398000\n",
      "398500\n",
      "399000\n",
      "399500\n",
      "400000\n",
      "Episode 800, Task 5/7, Reward: -500.0\n",
      "400500\n",
      "401000\n",
      "401500\n",
      "402000\n",
      "402500\n",
      "403000\n",
      "403500\n",
      "404000\n",
      "404500\n",
      "405000\n",
      "405500\n",
      "406000\n",
      "406500\n",
      "407000\n",
      "407500\n",
      "408000\n",
      "408500\n",
      "409000\n",
      "409500\n",
      "410000\n",
      "Episode 820, Task 3/7, Reward: -500.0\n",
      "410500\n",
      "411000\n",
      "411500\n",
      "412000\n",
      "412500\n",
      "413000\n",
      "413500\n",
      "414000\n",
      "414500\n",
      "415000\n",
      "415500\n",
      "416000\n",
      "416500\n",
      "417000\n",
      "417500\n",
      "418000\n",
      "418500\n",
      "419000\n",
      "419500\n",
      "420000\n",
      "Episode 840, Task 4/7, Reward: -500.0\n",
      "420500\n",
      "421000\n",
      "421500\n",
      "422000\n",
      "422500\n",
      "423000\n",
      "423500\n",
      "424000\n",
      "424500\n",
      "425000\n",
      "425500\n",
      "426000\n",
      "426500\n",
      "427000\n",
      "427500\n",
      "428000\n",
      "428500\n",
      "429000\n",
      "429500\n",
      "430000\n",
      "Episode 860, Task 2/7, Reward: -500.0\n",
      "430500\n",
      "431000\n",
      "431500\n",
      "432000\n",
      "432500\n",
      "433000\n",
      "433500\n",
      "434000\n",
      "434500\n",
      "435000\n",
      "435500\n",
      "436000\n",
      "436500\n",
      "437000\n",
      "437500\n",
      "438000\n",
      "438500\n",
      "439000\n",
      "439500\n",
      "440000\n",
      "Episode 880, Task 7/7, Reward: -500.0\n",
      "440500\n",
      "441000\n",
      "441500\n",
      "442000\n",
      "442500\n",
      "443000\n",
      "443500\n",
      "444000\n",
      "444500\n",
      "445000\n",
      "445500\n",
      "446000\n",
      "446500\n",
      "447000\n",
      "447500\n",
      "448000\n",
      "448500\n",
      "449000\n",
      "449500\n",
      "450000\n",
      "Episode 900, Task 5/7, Reward: -500.0\n",
      "450500\n",
      "451000\n",
      "451500\n",
      "452000\n",
      "452500\n",
      "453000\n",
      "453500\n",
      "454000\n",
      "454500\n",
      "455000\n",
      "455500\n",
      "456000\n",
      "456500\n",
      "457000\n",
      "457500\n",
      "458000\n",
      "458500\n",
      "459000\n",
      "459500\n",
      "460000\n",
      "Episode 920, Task 3/7, Reward: -500.0\n",
      "460500\n",
      "461000\n",
      "461500\n",
      "462000\n",
      "462500\n",
      "463000\n",
      "463500\n",
      "464000\n",
      "464500\n",
      "465000\n",
      "465500\n",
      "466000\n",
      "466500\n",
      "467000\n",
      "467500\n",
      "468000\n",
      "468500\n",
      "469000\n",
      "469500\n",
      "470000\n",
      "Episode 940, Task 2/7, Reward: -500.0\n",
      "470500\n",
      "471000\n",
      "471500\n",
      "472000\n",
      "472500\n",
      "473000\n",
      "473500\n",
      "474000\n",
      "474500\n",
      "475000\n",
      "475500\n",
      "476000\n",
      "476500\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return (\n",
    "            np.array(states), \n",
    "            np.array(actions), \n",
    "            np.array(rewards), \n",
    "            np.array(next_states), \n",
    "            np.array(dones)\n",
    "        )\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def train_agent(model, train_tasks, episodes=2000, lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.1,epsilon_decay=0.995, batch_size=64, buffer_capacity=50000, update_freq=4, max_steps=500):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "    epsilon = epsilon_start\n",
    "    step_count = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Randomly sample a task at each episode\n",
    "        task_idx = np.random.randint(0, len(train_tasks))\n",
    "        gravity, height, slope = train_tasks[task_idx]\n",
    "        env = ContinuousMountainCarCustom(gravity, height, slope)\n",
    "        env = NormalizeEnv(env)\n",
    "\n",
    "        state = env.reset()\n",
    "        state = np.concatenate([state, [gravity, height, slope]])  # Include task-specific parameters\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = model(torch.FloatTensor(state)).argmax().item()\n",
    "\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            next_state = np.concatenate([next_state, [gravity, height, slope]])  # Include task-specific parameters\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Perform training on the replay buffer\n",
    "            if step_count % update_freq == 0 and replay_buffer.size() > batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                states = torch.FloatTensor(states)\n",
    "                actions = torch.LongTensor(actions)\n",
    "                rewards = torch.FloatTensor(rewards)\n",
    "                next_states = torch.FloatTensor(next_states)\n",
    "                dones = torch.FloatTensor(dones)\n",
    "    \n",
    "                q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                next_q_values = model(next_states).max(1)[0]\n",
    "                target_q_values = rewards + (1 - dones) * gamma * next_q_values\n",
    "    \n",
    "                loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Reduce epsilon\n",
    "                epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        print(step_count)\n",
    "        if (episode + 1) % 20 == 0:\n",
    "            print(f\"Episode {episode + 1}, Task {task_idx + 1}/{len(train_tasks)}, Reward: {episode_reward}\")\n",
    "\n",
    "# Test the agent\n",
    "def test_agent(model, test_tasks, episodes=100):\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        # Randomly sample a task at each episode\n",
    "        task_idx = np.random.randint(0, len(test_tasks))\n",
    "        gravity, height, slope = test_tasks[task_idx]\n",
    "        env = ContinuousMountainCarCustom(gravity, height, slope)\n",
    "        env = NormalizeEnv(env)\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.concatenate([state, [gravity, height, slope]])  # Include task-specific parameters\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "            \n",
    "        while not done:\n",
    "            action = model(torch.FloatTensor(state)).argmax().item()\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            next_state = np.concatenate([next_state, [gravity, height, slope]])  # Include task-specific parameters\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "        print(f\"Task {task_idx + 1}, Episode {episode + 1}, Reward: {episode_reward}\")\n",
    "    \n",
    "    average_reward = np.mean(rewards)\n",
    "    print(f\"Average Reward across all test tasks: {average_reward}\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    state_dim = 2 + 3  # State features + task-specific parameters (gravity, height, slope)\n",
    "    action_dim = 3  # Continuous Mountain Car has 3 discrete actions\n",
    "\n",
    "    model = DQN(state_dim, action_dim)\n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "    train_agent(model, train_tasks)\n",
    "\n",
    "    print(\"\\nStarting Testing...\")\n",
    "    test_agent(model, test_tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e1e40-408a-45a1-bc69-b27d388c0e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c2500-e510-4baf-a394-478e3301b55b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
