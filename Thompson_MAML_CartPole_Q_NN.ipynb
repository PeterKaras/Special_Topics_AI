{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFfNBskxxkdt",
        "outputId": "77f28c20-935e-48df-a78f-259f3f7e7286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1/5\n",
            "Training Task 1/5\n",
            "Task 1, Episode 10, Average Score: 10.00\n",
            "Task 1, Episode 20, Average Score: 18.00\n",
            "Task 1, Episode 30, Average Score: 33.20\n",
            "Task 1, Episode 40, Average Score: 11.40\n",
            "Task 1, Episode 50, Average Score: 10.20\n",
            "Task 1, Episode 60, Average Score: 15.80\n",
            "Task 1, Episode 70, Average Score: 15.70\n",
            "Task 1, Episode 80, Average Score: 22.60\n",
            "Task 1, Episode 90, Average Score: 37.40\n",
            "Task 1, Episode 100, Average Score: 23.70\n",
            "Task 1, Episode 110, Average Score: 47.30\n",
            "Task 1, Episode 120, Average Score: 35.40\n",
            "Task 1, Episode 130, Average Score: 34.60\n",
            "Task 1, Episode 140, Average Score: 38.70\n",
            "Task 1, Episode 150, Average Score: 72.40\n",
            "Task 1, Episode 160, Average Score: 110.00\n",
            "Task 1, Episode 170, Average Score: 73.80\n",
            "Task 1, Episode 180, Average Score: 73.00\n",
            "Task 1, Episode 190, Average Score: 63.80\n",
            "Task 1, Episode 200, Average Score: 154.90\n",
            "Task 1, Episode 210, Average Score: 126.80\n",
            "Task 1, Episode 220, Average Score: 116.20\n",
            "Task 1, Episode 230, Average Score: 181.10\n",
            "Task 1, Episode 240, Average Score: 213.30\n",
            "Task 1, Episode 250, Average Score: 247.50\n",
            "Task 1, Episode 260, Average Score: 165.30\n",
            "Task 1, Episode 270, Average Score: 234.00\n",
            "Task 1, Episode 280, Average Score: 260.00\n",
            "Task 1, Episode 290, Average Score: 303.30\n",
            "Task 1, Episode 300, Average Score: 235.60\n",
            "Task 1, Episode 310, Average Score: 168.10\n",
            "Task 1, Episode 320, Average Score: 278.40\n",
            "Task 1, Episode 330, Average Score: 210.50\n",
            "Task 1, Episode 340, Average Score: 329.10\n",
            "Task 1, Episode 350, Average Score: 294.00\n",
            "Task 1, Episode 360, Average Score: 195.10\n",
            "Task 1, Episode 370, Average Score: 168.40\n",
            "Task 1, Episode 380, Average Score: 296.50\n",
            "Task 1, Episode 390, Average Score: 241.80\n",
            "Task 1, Episode 400, Average Score: 171.30\n",
            "Task 1, Episode 410, Average Score: 226.40\n",
            "Task 1, Episode 420, Average Score: 184.80\n",
            "Task 1, Episode 430, Average Score: 140.90\n",
            "Task 1, Episode 440, Average Score: 115.60\n",
            "Task 1, Episode 450, Average Score: 102.00\n",
            "Task 1, Episode 460, Average Score: 65.40\n",
            "Task 1, Episode 470, Average Score: 117.10\n",
            "Task 1, Episode 480, Average Score: 103.40\n",
            "Task 1, Episode 490, Average Score: 110.00\n",
            "Task 1, Episode 500, Average Score: 37.70\n",
            "Training Task 2/5\n",
            "Task 2, Episode 10, Average Score: 101.90\n",
            "Task 2, Episode 20, Average Score: 172.70\n",
            "Task 2, Episode 30, Average Score: 109.90\n",
            "Task 2, Episode 40, Average Score: 124.10\n",
            "Task 2, Episode 50, Average Score: 201.60\n",
            "Task 2, Episode 60, Average Score: 174.00\n",
            "Task 2, Episode 70, Average Score: 290.00\n",
            "Task 2, Episode 80, Average Score: 199.50\n",
            "Task 2, Episode 90, Average Score: 199.40\n",
            "Task 2, Episode 100, Average Score: 254.70\n",
            "Task 2, Episode 110, Average Score: 230.00\n",
            "Task 2, Episode 120, Average Score: 322.00\n",
            "Task 2, Episode 130, Average Score: 266.30\n",
            "Task 2, Episode 140, Average Score: 258.00\n",
            "Task 2, Episode 150, Average Score: 192.10\n",
            "Task 2, Episode 160, Average Score: 228.70\n",
            "Task 2, Episode 170, Average Score: 296.80\n",
            "Task 2, Episode 180, Average Score: 371.30\n",
            "Task 2, Episode 190, Average Score: 282.50\n",
            "Task 2, Episode 200, Average Score: 243.00\n",
            "Task 2, Episode 210, Average Score: 153.20\n",
            "Task 2, Episode 220, Average Score: 131.20\n",
            "Task 2, Episode 230, Average Score: 235.10\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "from collections import deque, OrderedDict\n",
        "import random\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)  # Outputs Q-values for each action\n",
        "\n",
        "\n",
        "class MetaQPolicy:\n",
        "    def __init__(self, policy):\n",
        "        self.policy = policy\n",
        "        self.params = OrderedDict(policy.named_parameters())\n",
        "\n",
        "    def update_params(self, loss, step_size=0.1, first_order=False):\n",
        "        grads = torch.autograd.grad(loss, self.params.values(), create_graph=not first_order)\n",
        "\n",
        "        updated_params = OrderedDict()\n",
        "        for (name, param), grad in zip(self.params.items(), grads):\n",
        "            updated_params[name] = param - step_size * grad.clamp(-5, 5)  # Gradient clipping\n",
        "        return updated_params\n",
        "\n",
        "    def set_params(self, params):\n",
        "        for name, param in self.policy.named_parameters():\n",
        "            param.data = params[name].data.clone()\n",
        "\n",
        "class ThompsonMetaQLearning:\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=224, inner_lr=0.0003590122649502572, meta_lr=0.009767756309404269,\n",
        "                 memory_size=1000, batch_size=64, gamma=0.99, epsilon=0.1, thompson_alpha=1.5540433190233864, thompson_beta=0.9897499530825413):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.q_network = QNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
        "        self.meta_policy = MetaQPolicy(self.q_network)\n",
        "        self.target_network = QNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "\n",
        "        self.meta_optimizer = optim.Adam(self.q_network.parameters(), lr=meta_lr)\n",
        "        self.task_memories = {}  # Dictionary to store memory for each task\n",
        "        self.memory_size = memory_size\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.inner_lr = inner_lr\n",
        "\n",
        "        # Thompson sampling parameters\n",
        "        self.alpha = np.ones(action_dim) * thompson_alpha\n",
        "        self.beta = np.ones(action_dim) * thompson_beta\n",
        "\n",
        "    def create_task_memory(self, task_id):\n",
        "        \"\"\"Initialize memory for a new task.\"\"\"\n",
        "        if task_id not in self.task_memories:\n",
        "            self.task_memories[task_id] = deque(maxlen=self.memory_size)\n",
        "\n",
        "    def store_transition(self, task_id, state, action, reward, next_state, done):\n",
        "        \"\"\"Store transitions in task-specific memory.\"\"\"\n",
        "        if task_id not in self.task_memories:\n",
        "            self.create_task_memory(task_id)\n",
        "        self.task_memories[task_id].append((state, action, reward, next_state, done))\n",
        "\n",
        "        # Update Thompson sampling parameters\n",
        "        if reward > 0:\n",
        "            self.alpha[action] += 1\n",
        "        else:\n",
        "            self.beta[action] += 1\n",
        "\n",
        "    def prepare_batch(self, batch):\n",
        "        \"\"\"Prepare batches for training.\"\"\"\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def inner_loop_update(self, task_id):\n",
        "        \"\"\"Perform inner-loop update on task-specific memory.\"\"\"\n",
        "        memory = self.task_memories[task_id]\n",
        "        if len(memory) < self.batch_size:\n",
        "            return None\n",
        "\n",
        "        batch = random.sample(memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = self.prepare_batch(batch)\n",
        "\n",
        "        # Compute Q-values\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "        with torch.no_grad():\n",
        "            max_next_q_values = self.target_network(next_states).max(1)[0]\n",
        "            target_q_values = rewards + self.gamma * (1 - dones) * max_next_q_values\n",
        "\n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(q_values, target_q_values)\n",
        "\n",
        "        # Update parameters\n",
        "        updated_params = self.meta_policy.update_params(loss, self.inner_lr)\n",
        "        return updated_params\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using Thompson sampling and epsilon-greedy exploration.\"\"\"\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # Sample Thompson probabilities\n",
        "        with torch.no_grad():\n",
        "            q_values = self.q_network(state).cpu().numpy()[0]\n",
        "        thompson_probs = np.array([np.random.beta(self.alpha[i], self.beta[i]) for i in range(len(q_values))])\n",
        "        weighted_q_values = q_values * thompson_probs\n",
        "\n",
        "        # Epsilon-greedy exploration\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(len(q_values))\n",
        "        return np.argmax(weighted_q_values)\n",
        "\n",
        "    def meta_update(self):\n",
        "        \"\"\"Perform meta-update using memories of all tasks.\"\"\"\n",
        "        if len(self.task_memories) == 0:\n",
        "            return\n",
        "\n",
        "        self.meta_optimizer.zero_grad()\n",
        "        meta_loss = 0\n",
        "\n",
        "        # Iterate over each task's memory for meta-learning\n",
        "        for task_id, memory in self.task_memories.items():\n",
        "            if len(memory) < self.batch_size * 2:\n",
        "                continue\n",
        "\n",
        "            # Sample task and evaluation batches\n",
        "            task_batch = random.sample(memory, self.batch_size)\n",
        "            eval_batch = random.sample(memory, self.batch_size)\n",
        "\n",
        "            updated_params = self.inner_loop_update(task_id)\n",
        "\n",
        "            if updated_params is None:\n",
        "                continue\n",
        "\n",
        "            # Temporarily set updated parameters for evaluation\n",
        "            original_params = OrderedDict((name, param.clone()) for name, param in self.q_network.named_parameters())\n",
        "            self.meta_policy.set_params(updated_params)\n",
        "\n",
        "            # Evaluate on evaluation batch\n",
        "            states, actions, rewards, next_states, dones = self.prepare_batch(eval_batch)\n",
        "            q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "            with torch.no_grad():\n",
        "                max_next_q_values = self.target_network(next_states).max(1)[0]\n",
        "                target_q_values = rewards + self.gamma * (1 - dones) * max_next_q_values\n",
        "\n",
        "            # Compute meta-loss\n",
        "            task_meta_loss = F.mse_loss(q_values, target_q_values)\n",
        "            meta_loss += task_meta_loss\n",
        "\n",
        "            # Revert to original parameters\n",
        "            self.meta_policy.set_params(original_params)\n",
        "\n",
        "        if meta_loss > 0:\n",
        "            meta_loss = meta_loss / len(self.task_memories)\n",
        "            meta_loss.backward()\n",
        "            self.meta_optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Update target network with the main Q-network's weights.\"\"\"\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "\n",
        "def train_thompson_meta_q(env_name='CartPole-v1', tasks=5, episodes=1000, max_steps=500):\n",
        "    \"\"\"Train Thompson Meta Q-learning with multiple tasks.\"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    agent = ThompsonMetaQLearning(state_dim, action_dim)\n",
        "    scores_per_task = {task_id: [] for task_id in range(tasks)}\n",
        "\n",
        "    for task_id in range(tasks):\n",
        "        print(f\"Training Task {task_id + 1}/{tasks}\")\n",
        "        for episode in range(episodes):\n",
        "            state = env.reset()\n",
        "            total_reward = 0\n",
        "\n",
        "            for _ in range(max_steps):\n",
        "                action = agent.select_action(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                agent.store_transition(task_id, state, action, reward, next_state, done)\n",
        "                total_reward += reward\n",
        "                state = next_state\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            scores_per_task[task_id].append(total_reward)\n",
        "            agent.meta_update()\n",
        "\n",
        "            # Update target network periodically\n",
        "            if episode % 10 == 0:\n",
        "                agent.update_target_network()\n",
        "\n",
        "            # Decay epsilon\n",
        "            agent.epsilon = max(0.1, agent.epsilon * 0.99)\n",
        "\n",
        "            if (episode + 1) % 10 == 0:\n",
        "                avg_score = np.mean(scores_per_task[task_id][-10:])\n",
        "                print(f\"Task {task_id + 1}, Episode {episode + 1}, Average Score: {avg_score:.2f}\")\n",
        "\n",
        "    return agent, scores_per_task\n",
        "\n",
        "\n",
        "def run_thompson_meta_q_multiple_times(env_name='CartPole-v1', runs=50, episodes=500, max_steps=500):\n",
        "    all_scores = []\n",
        "    all_losses = []  # Placeholder for loss tracking\n",
        "\n",
        "    for run in range(runs):\n",
        "        print(f\"Run {run + 1}/{runs}\")\n",
        "        agent, scores = train_thompson_meta_q(env_name=env_name, episodes=episodes, max_steps=max_steps)\n",
        "        all_scores.append(scores)\n",
        "        # Placeholder: You can implement loss tracking in `train_thompson_meta_q` and collect here.\n",
        "        # all_losses.append(losses)\n",
        "\n",
        "    # Pad the scores to equal lengths with NaNs for aggregation\n",
        "    max_len = max(len(scores) for scores in all_scores)\n",
        "    padded_scores = np.array([np.pad(scores, (0, max_len - len(scores)), constant_values=np.nan) for scores in all_scores])\n",
        "\n",
        "    # Compute statistics\n",
        "    median_scores = np.nanmedian(padded_scores, axis=0)\n",
        "    mean_scores = np.nanmean(padded_scores, axis=0)\n",
        "    std_scores = np.nanstd(padded_scores, axis=0)\n",
        "\n",
        "    return {\n",
        "        'all_scores': all_scores,\n",
        "        'median_scores': median_scores,\n",
        "        'mean_scores': mean_scores,\n",
        "        'std_scores': std_scores,\n",
        "        'padded_scores': padded_scores\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(1)\n",
        "    np.random.seed(1)\n",
        "    random.seed(1)\n",
        "\n",
        "    stats = run_thompson_meta_q_multiple_times(runs=5, episodes=500)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot median and mean scores with confidence intervals\n",
        "    x = np.arange(len(stats['median_scores']))\n",
        "    plt.plot(stats['median_scores'], label='Median Reward', color='blue')\n",
        "    plt.plot(stats['mean_scores'], label='Mean Reward', color='orange')\n",
        "    plt.fill_between(x,\n",
        "                     stats['mean_scores'] - stats['std_scores'],\n",
        "                     stats['mean_scores'] + stats['std_scores'],\n",
        "                     color='orange', alpha=0.2, label='Â±1 Std Dev')\n",
        "\n",
        "    plt.title('Thompson Meta Q-Learning Across Multiple Runs')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NE6anicNEPLN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ccf838d6-cdee-48fd-d0f6-973fbd9a82ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-06 09:03:35,111] A new study created in memory with name: no-name-7fd91c07-45d9-43c3-8c20-3ee061952640\n",
            "<ipython-input-15-20944992d631>:8: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-15-20944992d631>:9: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-15-20944992d631>:10: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "<ipython-input-15-20944992d631>:11: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "<ipython-input-15-20944992d631>:12: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "<ipython-input-15-20944992d631>:15: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning:\n",
            "\n",
            "\u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning:\n",
            "\n",
            "\u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning:\n",
            "\n",
            "`np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "\n",
            "[I 2024-12-06 09:04:10,411] Trial 0 finished with value: 174.4 and parameters: {'hidden_dim': 128, 'inner_lr': 6.130063411093321e-05, 'meta_lr': 0.007506141259463613, 'thompson_alpha': 0.39438338721893573, 'thompson_beta': 1.2835541471329863, 'epsilon_decay': 0.9586416357395998, 'batch_size': 224, 'memory_size': 6500, 'gamma': 0.9979454495141876}. Best is trial 0 with value: 174.4.\n",
            "[I 2024-12-06 09:04:30,118] Trial 1 finished with value: 137.0 and parameters: {'hidden_dim': 64, 'inner_lr': 0.009750370502686406, 'meta_lr': 0.0008188030183210543, 'thompson_alpha': 1.9968666354911704, 'thompson_beta': 1.7416299128360828, 'epsilon_decay': 0.9860698624410269, 'batch_size': 232, 'memory_size': 7800, 'gamma': 0.9113778852457863}. Best is trial 0 with value: 174.4.\n",
            "[I 2024-12-06 09:04:42,955] Trial 2 finished with value: 10.3 and parameters: {'hidden_dim': 256, 'inner_lr': 8.297423516265734e-05, 'meta_lr': 1.4122235925459312e-05, 'thompson_alpha': 0.8728216424447863, 'thompson_beta': 0.751279755356971, 'epsilon_decay': 0.9524836072543246, 'batch_size': 104, 'memory_size': 1800, 'gamma': 0.9282136444794784}. Best is trial 0 with value: 174.4.\n",
            "[I 2024-12-06 09:05:14,341] Trial 3 finished with value: 246.8 and parameters: {'hidden_dim': 224, 'inner_lr': 0.00022522743567510284, 'meta_lr': 0.0013703992973783216, 'thompson_alpha': 1.6401829411193696, 'thompson_beta': 1.606202537903854, 'epsilon_decay': 0.9766096343529173, 'batch_size': 224, 'memory_size': 9400, 'gamma': 0.9343636543955004}. Best is trial 3 with value: 246.8.\n",
            "[I 2024-12-06 09:05:39,891] Trial 4 finished with value: 161.0 and parameters: {'hidden_dim': 96, 'inner_lr': 0.008625288661026366, 'meta_lr': 0.004929843837401972, 'thompson_alpha': 1.0098872443709777, 'thompson_beta': 0.7384380079964676, 'epsilon_decay': 0.9827891784318075, 'batch_size': 48, 'memory_size': 4500, 'gamma': 0.926422562319314}. Best is trial 3 with value: 246.8.\n",
            "[I 2024-12-06 09:05:53,038] Trial 5 finished with value: 20.5 and parameters: {'hidden_dim': 64, 'inner_lr': 0.0006737229232421647, 'meta_lr': 0.00043244838461645375, 'thompson_alpha': 1.436235356632011, 'thompson_beta': 1.9754896180580706, 'epsilon_decay': 0.9646705879955209, 'batch_size': 72, 'memory_size': 5100, 'gamma': 0.9114422440827191}. Best is trial 3 with value: 246.8.\n",
            "[I 2024-12-06 09:06:07,980] Trial 6 finished with value: 86.9 and parameters: {'hidden_dim': 224, 'inner_lr': 0.0008722297322059646, 'meta_lr': 0.000418539914219207, 'thompson_alpha': 1.0655569381349064, 'thompson_beta': 1.8592843969957895, 'epsilon_decay': 0.9984608305273119, 'batch_size': 16, 'memory_size': 6500, 'gamma': 0.9483471893167058}. Best is trial 3 with value: 246.8.\n",
            "[I 2024-12-06 09:06:19,131] Trial 7 finished with value: 10.0 and parameters: {'hidden_dim': 32, 'inner_lr': 0.0004580132273144216, 'meta_lr': 0.00020203638050416567, 'thompson_alpha': 1.738092721468483, 'thompson_beta': 0.9066520935068325, 'epsilon_decay': 0.9608872825086691, 'batch_size': 16, 'memory_size': 5000, 'gamma': 0.9406011828808496}. Best is trial 3 with value: 246.8.\n",
            "[I 2024-12-06 09:06:34,067] Trial 8 finished with value: 32.0 and parameters: {'hidden_dim': 64, 'inner_lr': 0.024098080781696774, 'meta_lr': 0.00026650515429551674, 'thompson_alpha': 1.3359378049174375, 'thompson_beta': 0.7058272103653849, 'epsilon_decay': 0.9731192826628946, 'batch_size': 240, 'memory_size': 1000, 'gamma': 0.9987410351270375}. Best is trial 3 with value: 246.8.\n",
            "[I 2024-12-06 09:06:45,815] Trial 9 finished with value: 9.9 and parameters: {'hidden_dim': 96, 'inner_lr': 0.0013179320757896764, 'meta_lr': 0.0001257123002708998, 'thompson_alpha': 1.6980574161127737, 'thompson_beta': 0.35925413125326033, 'epsilon_decay': 0.9571930142617214, 'batch_size': 56, 'memory_size': 9100, 'gamma': 0.9385039451127484}. Best is trial 3 with value: 246.8.\n",
            "[I 2024-12-06 09:07:17,031] Trial 10 finished with value: 139.0 and parameters: {'hidden_dim': 192, 'inner_lr': 1.5569667079159963e-05, 'meta_lr': 0.001743379405056645, 'thompson_alpha': 0.21275123826917264, 'thompson_beta': 1.1576116724415872, 'epsilon_decay': 0.9732057772269476, 'batch_size': 184, 'memory_size': 9500, 'gamma': 0.9710821278382141}. Best is trial 3 with value: 246.8.\n",
            "[I 2024-12-06 09:07:56,844] Trial 11 finished with value: 180.8 and parameters: {'hidden_dim': 160, 'inner_lr': 5.074248364173372e-05, 'meta_lr': 0.008129734807077604, 'thompson_alpha': 0.2566304386276302, 'thompson_beta': 1.4009384612078017, 'epsilon_decay': 0.9670484411113393, 'batch_size': 176, 'memory_size': 7500, 'gamma': 0.9681175143797586}. Best is trial 3 with value: 246.8.\n",
            "[I 2024-12-06 09:08:27,694] Trial 12 finished with value: 135.3 and parameters: {'hidden_dim': 192, 'inner_lr': 9.656567008708578e-05, 'meta_lr': 0.0024462823367224604, 'thompson_alpha': 0.6621951158837127, 'thompson_beta': 1.4873427464367297, 'epsilon_decay': 0.9684312872561566, 'batch_size': 168, 'memory_size': 8200, 'gamma': 0.9734788361781905}. Best is trial 3 with value: 246.8.\n",
            "[I 2024-12-06 09:08:57,724] Trial 13 finished with value: 277.4 and parameters: {'hidden_dim': 160, 'inner_lr': 1.5163123349065405e-05, 'meta_lr': 0.0023345150725141307, 'thompson_alpha': 0.5533682541374254, 'thompson_beta': 1.4998483451305327, 'epsilon_decay': 0.9818416856250997, 'batch_size': 168, 'memory_size': 9900, 'gamma': 0.9691860911361538}. Best is trial 13 with value: 277.4.\n",
            "[I 2024-12-06 09:09:35,014] Trial 14 finished with value: 349.4 and parameters: {'hidden_dim': 256, 'inner_lr': 2.8918258488766246e-05, 'meta_lr': 0.0014621972769377782, 'thompson_alpha': 0.5349511190168977, 'thompson_beta': 1.6144110496984403, 'epsilon_decay': 0.9831446925112605, 'batch_size': 128, 'memory_size': 9900, 'gamma': 0.9611971737120122}. Best is trial 14 with value: 349.4.\n",
            "[I 2024-12-06 09:09:48,261] Trial 15 finished with value: 12.1 and parameters: {'hidden_dim': 256, 'inner_lr': 1.2908447539893056e-05, 'meta_lr': 9.121730483478708e-05, 'thompson_alpha': 0.49636882065741006, 'thompson_beta': 1.63423794124565, 'epsilon_decay': 0.9900513831467653, 'batch_size': 128, 'memory_size': 9900, 'gamma': 0.9610480436324147}. Best is trial 14 with value: 349.4.\n",
            "[I 2024-12-06 09:10:22,610] Trial 16 finished with value: 194.7 and parameters: {'hidden_dim': 160, 'inner_lr': 2.5526564898492037e-05, 'meta_lr': 0.003129828192731563, 'thompson_alpha': 0.6874764077249989, 'thompson_beta': 1.1164713003324958, 'epsilon_decay': 0.9937045624531106, 'batch_size': 144, 'memory_size': 3200, 'gamma': 0.9820068866065278}. Best is trial 14 with value: 349.4.\n",
            "[I 2024-12-06 09:10:47,959] Trial 17 finished with value: 314.2 and parameters: {'hidden_dim': 192, 'inner_lr': 1.1620325893113931e-05, 'meta_lr': 0.0008860077274711712, 'thompson_alpha': 0.1271568661453924, 'thompson_beta': 0.16227191154860632, 'epsilon_decay': 0.9799799852751901, 'batch_size': 104, 'memory_size': 8500, 'gamma': 0.956244390179818}. Best is trial 14 with value: 349.4.\n",
            "[I 2024-12-06 09:11:33,473] Trial 18 finished with value: 427.6 and parameters: {'hidden_dim': 224, 'inner_lr': 0.0023555612297509435, 'meta_lr': 0.0007547791290716876, 'thompson_alpha': 0.12229960866338863, 'thompson_beta': 0.15587124814800035, 'epsilon_decay': 0.9785167878152308, 'batch_size': 96, 'memory_size': 6800, 'gamma': 0.9532169167243258}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:11:47,744] Trial 19 finished with value: 15.3 and parameters: {'hidden_dim': 224, 'inner_lr': 0.08964311289259963, 'meta_lr': 5.500973031064123e-05, 'thompson_alpha': 0.3381042670537703, 'thompson_beta': 0.5273852617644547, 'epsilon_decay': 0.9922530493470904, 'batch_size': 96, 'memory_size': 6100, 'gamma': 0.9503095554818728}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:12:17,163] Trial 20 finished with value: 296.5 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0040492581978584805, 'meta_lr': 0.0008002605829604258, 'thompson_alpha': 0.11596998288471083, 'thompson_beta': 0.22746907260284288, 'epsilon_decay': 0.9865692870180697, 'batch_size': 128, 'memory_size': 3500, 'gamma': 0.985846195666587}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:12:41,908] Trial 21 finished with value: 48.5 and parameters: {'hidden_dim': 192, 'inner_lr': 0.002731762657316147, 'meta_lr': 0.0008764611165169877, 'thompson_alpha': 0.1540624826985227, 'thompson_beta': 0.11750579029704421, 'epsilon_decay': 0.9779827290139171, 'batch_size': 96, 'memory_size': 8500, 'gamma': 0.9553418754281704}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:13:04,880] Trial 22 finished with value: 305.2 and parameters: {'hidden_dim': 224, 'inner_lr': 0.00016965263045040543, 'meta_lr': 0.0006030912565754784, 'thompson_alpha': 0.42377785428855386, 'thompson_beta': 0.4367959204064071, 'epsilon_decay': 0.9799850443435152, 'batch_size': 80, 'memory_size': 7300, 'gamma': 0.9600597679105438}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:13:35,881] Trial 23 finished with value: 297.9 and parameters: {'hidden_dim': 192, 'inner_lr': 3.141517564590586e-05, 'meta_lr': 0.001434528748824134, 'thompson_alpha': 0.10493154420077928, 'thompson_beta': 0.13570048428734252, 'epsilon_decay': 0.9856876252476016, 'batch_size': 144, 'memory_size': 8700, 'gamma': 0.9481252887759791}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:14:09,435] Trial 24 finished with value: 189.2 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0001974274779987038, 'meta_lr': 0.003967826785180861, 'thompson_alpha': 0.7522559606156043, 'thompson_beta': 0.561882201550234, 'epsilon_decay': 0.9731856007430034, 'batch_size': 112, 'memory_size': 7100, 'gamma': 0.9604558117960545}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:14:38,973] Trial 25 finished with value: 167.8 and parameters: {'hidden_dim': 224, 'inner_lr': 0.002073315642001451, 'meta_lr': 0.0011199249743287273, 'thompson_alpha': 0.33324465249643465, 'thompson_beta': 0.9772266667502366, 'epsilon_decay': 0.9697979664593215, 'batch_size': 48, 'memory_size': 8200, 'gamma': 0.9782584980456597}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:14:52,982] Trial 26 finished with value: 49.3 and parameters: {'hidden_dim': 192, 'inner_lr': 0.0003828027599826969, 'meta_lr': 0.00023798164965879238, 'thompson_alpha': 0.5408429190322201, 'thompson_beta': 0.3148510815101411, 'epsilon_decay': 0.9787333990054712, 'batch_size': 72, 'memory_size': 9000, 'gamma': 0.9427751990916103}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:15:16,150] Trial 27 finished with value: 173.9 and parameters: {'hidden_dim': 256, 'inner_lr': 3.265986603434491e-05, 'meta_lr': 0.00053344747208389, 'thompson_alpha': 0.261525499855543, 'thompson_beta': 0.24626213812116304, 'epsilon_decay': 0.9831366284171493, 'batch_size': 120, 'memory_size': 6100, 'gamma': 0.9196193063558338}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:15:29,147] Trial 28 finished with value: 9.3 and parameters: {'hidden_dim': 224, 'inner_lr': 0.007630185797784164, 'meta_lr': 4.521783323555286e-05, 'thompson_alpha': 0.8365167868323528, 'thompson_beta': 0.6024740254977501, 'epsilon_decay': 0.9882749729761835, 'batch_size': 144, 'memory_size': 6900, 'gamma': 0.9528491629124578}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:15:42,904] Trial 29 finished with value: 9.6 and parameters: {'hidden_dim': 160, 'inner_lr': 0.043430245293617074, 'meta_lr': 0.004239730815401127, 'thompson_alpha': 0.3874612684177503, 'thompson_beta': 0.9072015912712453, 'epsilon_decay': 0.9963790958443672, 'batch_size': 192, 'memory_size': 7900, 'gamma': 0.9635513325309193}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:16:17,514] Trial 30 finished with value: 293.8 and parameters: {'hidden_dim': 128, 'inner_lr': 5.2052977217279945e-05, 'meta_lr': 0.005775316464296799, 'thompson_alpha': 0.5882313566289983, 'thompson_beta': 1.2311027203667733, 'epsilon_decay': 0.9757543697793006, 'batch_size': 200, 'memory_size': 6000, 'gamma': 0.98875300839171}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:16:38,664] Trial 31 finished with value: 247.2 and parameters: {'hidden_dim': 224, 'inner_lr': 0.0001532599473114547, 'meta_lr': 0.000673784404835888, 'thompson_alpha': 0.4325114762213063, 'thompson_beta': 0.4364001296864486, 'epsilon_decay': 0.9806814769452895, 'batch_size': 80, 'memory_size': 7200, 'gamma': 0.9569948059528756}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:16:53,728] Trial 32 finished with value: 18.8 and parameters: {'hidden_dim': 224, 'inner_lr': 1.0458548392039889e-05, 'meta_lr': 0.0003743241865163172, 'thompson_alpha': 0.4148438556047054, 'thompson_beta': 0.42408975792053616, 'epsilon_decay': 0.9843744785578278, 'batch_size': 88, 'memory_size': 7600, 'gamma': 0.9666870981952481}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:17:16,627] Trial 33 finished with value: 99.2 and parameters: {'hidden_dim': 256, 'inner_lr': 8.60276151572212e-05, 'meta_lr': 0.0006460428512933019, 'thompson_alpha': 0.2536133965079089, 'thompson_beta': 0.13501703350075145, 'epsilon_decay': 0.979836831303003, 'batch_size': 104, 'memory_size': 6600, 'gamma': 0.9759588622640949}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:17:43,636] Trial 34 finished with value: 272.2 and parameters: {'hidden_dim': 192, 'inner_lr': 2.5385233925720088e-05, 'meta_lr': 0.0011045624246006453, 'thompson_alpha': 0.3041713918418678, 'thompson_beta': 0.2815918398212737, 'epsilon_decay': 0.9761771175119428, 'batch_size': 64, 'memory_size': 8700, 'gamma': 0.9450326198423373}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:18:25,068] Trial 35 finished with value: 323.2 and parameters: {'hidden_dim': 256, 'inner_lr': 5.063724917257396e-05, 'meta_lr': 0.001918725612719592, 'thompson_alpha': 0.9774027151304183, 'thompson_beta': 0.44811233679170215, 'epsilon_decay': 0.9891377317979632, 'batch_size': 152, 'memory_size': 9300, 'gamma': 0.9358785621322055}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:19:08,575] Trial 36 finished with value: 334.4 and parameters: {'hidden_dim': 256, 'inner_lr': 4.9080278218703106e-05, 'meta_lr': 0.0020193789068864163, 'thompson_alpha': 1.1310055131778676, 'thompson_beta': 0.20724537633310003, 'epsilon_decay': 0.991533024667786, 'batch_size': 216, 'memory_size': 10000, 'gamma': 0.9325895014435281}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:19:40,642] Trial 37 finished with value: 330.4 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0003943703392402915, 'meta_lr': 0.0019468435233761122, 'thompson_alpha': 1.1087179851934879, 'thompson_beta': 0.6584395316094854, 'epsilon_decay': 0.9910111974063575, 'batch_size': 216, 'memory_size': 9400, 'gamma': 0.9304015225536713}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:20:23,233] Trial 38 finished with value: 161.6 and parameters: {'hidden_dim': 256, 'inner_lr': 0.00030862352022363745, 'meta_lr': 0.00245743155757213, 'thompson_alpha': 1.217890377586725, 'thompson_beta': 0.797395774590158, 'epsilon_decay': 0.993410065643371, 'batch_size': 256, 'memory_size': 10000, 'gamma': 0.9294851975886449}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:20:44,475] Trial 39 finished with value: 33.8 and parameters: {'hidden_dim': 256, 'inner_lr': 0.000710640461806657, 'meta_lr': 1.0317168850780798e-05, 'thompson_alpha': 1.1728610706707374, 'thompson_beta': 1.760600191476552, 'epsilon_decay': 0.9967496333926094, 'batch_size': 216, 'memory_size': 9500, 'gamma': 0.9182012096531859}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:21:17,701] Trial 40 finished with value: 328.2 and parameters: {'hidden_dim': 224, 'inner_lr': 0.001511996551838776, 'meta_lr': 0.009640177916779803, 'thompson_alpha': 1.543157597942217, 'thompson_beta': 0.8209622962876243, 'epsilon_decay': 0.987178731577332, 'batch_size': 208, 'memory_size': 4200, 'gamma': 0.9037098171092749}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:21:59,310] Trial 41 finished with value: 198.6 and parameters: {'hidden_dim': 224, 'inner_lr': 0.001238669842208435, 'meta_lr': 0.008506589482358691, 'thompson_alpha': 1.9447709046780153, 'thompson_beta': 0.630643741153367, 'epsilon_decay': 0.9911832224299851, 'batch_size': 208, 'memory_size': 3900, 'gamma': 0.9005276949603799}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:22:26,288] Trial 42 finished with value: 132.5 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0026229012498497385, 'meta_lr': 0.005814779437591718, 'thompson_alpha': 1.4565329089327623, 'thompson_beta': 0.7101315772662242, 'epsilon_decay': 0.9870221802606461, 'batch_size': 232, 'memory_size': 2600, 'gamma': 0.9043620898052558}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:22:43,463] Trial 43 finished with value: 9.7 and parameters: {'hidden_dim': 224, 'inner_lr': 0.005697867232458029, 'meta_lr': 0.003276270067704169, 'thompson_alpha': 1.5190297330032556, 'thompson_beta': 0.8090262959215201, 'epsilon_decay': 0.9947090412876995, 'batch_size': 240, 'memory_size': 4400, 'gamma': 0.9233895015697094}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:23:20,843] Trial 44 finished with value: 366.9 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0005983180438757192, 'meta_lr': 0.0015113289320791727, 'thompson_alpha': 1.2801064854613395, 'thompson_beta': 1.3021539292421804, 'epsilon_decay': 0.9843666871994705, 'batch_size': 256, 'memory_size': 5400, 'gamma': 0.9109196310803076}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:23:52,122] Trial 45 finished with value: 223.3 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0005153681139022012, 'meta_lr': 0.0015175106657262264, 'thompson_alpha': 1.2340831958873184, 'thompson_beta': 1.337724352513443, 'epsilon_decay': 0.9834954632743108, 'batch_size': 256, 'memory_size': 5000, 'gamma': 0.9313105093696005}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:24:13,239] Trial 46 finished with value: 100.2 and parameters: {'hidden_dim': 256, 'inner_lr': 0.020123240609798664, 'meta_lr': 0.0021245221591545877, 'thompson_alpha': 1.0918747904379142, 'thompson_beta': 1.8686768877645163, 'epsilon_decay': 0.9988467186767084, 'batch_size': 240, 'memory_size': 5800, 'gamma': 0.9138150025942849}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:24:28,556] Trial 47 finished with value: 40.9 and parameters: {'hidden_dim': 32, 'inner_lr': 0.00011786138060713053, 'meta_lr': 0.0012890158043946996, 'thompson_alpha': 1.315159952181682, 'thompson_beta': 1.5366469892744994, 'epsilon_decay': 0.9513988406509274, 'batch_size': 224, 'memory_size': 5200, 'gamma': 0.9338427652678211}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:25:07,968] Trial 48 finished with value: 255.4 and parameters: {'hidden_dim': 256, 'inner_lr': 0.00027931311347048973, 'meta_lr': 0.0030300094569315384, 'thompson_alpha': 0.9253170955257763, 'thompson_beta': 1.643714190059796, 'epsilon_decay': 0.9845018634267827, 'batch_size': 168, 'memory_size': 9600, 'gamma': 0.9243215395427159}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:25:24,712] Trial 49 finished with value: 34.8 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0008953200349645497, 'meta_lr': 0.00017424671835985798, 'thompson_alpha': 1.1004646368809232, 'thompson_beta': 1.4179197275298794, 'epsilon_decay': 0.9893541713603838, 'batch_size': 248, 'memory_size': 8900, 'gamma': 0.9078698955417517}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:25:38,577] Trial 50 finished with value: 9.9 and parameters: {'hidden_dim': 96, 'inner_lr': 0.0005523037394897586, 'meta_lr': 0.00033401173748288666, 'thompson_alpha': 1.3311956484883718, 'thompson_beta': 1.064941056799402, 'epsilon_decay': 0.9912294249455216, 'batch_size': 184, 'memory_size': 8100, 'gamma': 0.9390706305797027}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:26:04,673] Trial 51 finished with value: 190.4 and parameters: {'hidden_dim': 224, 'inner_lr': 0.0014218002303321177, 'meta_lr': 0.0017351111814542853, 'thompson_alpha': 1.5828662122639567, 'thompson_beta': 1.1802261581304934, 'epsilon_decay': 0.987697865936379, 'batch_size': 216, 'memory_size': 4500, 'gamma': 0.9155059107442374}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:26:34,465] Trial 52 finished with value: 9.9 and parameters: {'hidden_dim': 224, 'inner_lr': 0.003907692603214469, 'meta_lr': 0.0009922878320174926, 'thompson_alpha': 1.805609940985942, 'thompson_beta': 0.9052698913566777, 'epsilon_decay': 0.9827548586176451, 'batch_size': 200, 'memory_size': 4400, 'gamma': 0.9099207329846084}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:27:05,505] Trial 53 finished with value: 149.1 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0017286327363557085, 'meta_lr': 0.005007935161783685, 'thompson_alpha': 1.388810751711004, 'thompson_beta': 1.324404580245635, 'epsilon_decay': 0.9857748087097988, 'batch_size': 32, 'memory_size': 5500, 'gamma': 0.9219222665226031}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:27:40,525] Trial 54 finished with value: 157.1 and parameters: {'hidden_dim': 224, 'inner_lr': 0.0008797052690332829, 'meta_lr': 0.009977389652318, 'thompson_alpha': 1.0166949770398976, 'thompson_beta': 0.9754127916465741, 'epsilon_decay': 0.9909809905351474, 'batch_size': 224, 'memory_size': 2000, 'gamma': 0.900285722055381}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:28:00,681] Trial 55 finished with value: 10.9 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0032905031193090697, 'meta_lr': 0.0026553796470087797, 'thompson_alpha': 0.8668059411463502, 'thompson_beta': 0.3611669264212361, 'epsilon_decay': 0.9708169602736778, 'batch_size': 152, 'memory_size': 9600, 'gamma': 0.9050333402170877}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:28:23,151] Trial 56 finished with value: 315.2 and parameters: {'hidden_dim': 224, 'inner_lr': 0.012771460746139413, 'meta_lr': 0.00047879251024046405, 'thompson_alpha': 1.1436695925582834, 'thompson_beta': 0.20065087511855917, 'epsilon_decay': 0.9953346725630758, 'batch_size': 232, 'memory_size': 4000, 'gamma': 0.9127871886876291}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:28:44,169] Trial 57 finished with value: 310.4 and parameters: {'hidden_dim': 192, 'inner_lr': 1.9653618518699213e-05, 'meta_lr': 0.0007842583411591435, 'thompson_alpha': 1.2411313693579555, 'thompson_beta': 1.735336787022807, 'epsilon_decay': 0.956651486136049, 'batch_size': 208, 'memory_size': 10000, 'gamma': 0.9477680036477225}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:29:14,360] Trial 58 finished with value: 128.0 and parameters: {'hidden_dim': 128, 'inner_lr': 0.0011442761461938475, 'meta_lr': 0.0037426585747410137, 'thompson_alpha': 0.7941415902993061, 'thompson_beta': 0.6707499363276306, 'epsilon_decay': 0.9778397349933766, 'batch_size': 176, 'memory_size': 3100, 'gamma': 0.9280540192530361}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:29:43,008] Trial 59 finished with value: 258.9 and parameters: {'hidden_dim': 224, 'inner_lr': 7.213967910695702e-05, 'meta_lr': 0.0012903798189902828, 'thompson_alpha': 1.6096516103747673, 'thompson_beta': 0.795368790132812, 'epsilon_decay': 0.9813992856032424, 'batch_size': 120, 'memory_size': 9200, 'gamma': 0.9423028500792983}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:30:17,229] Trial 60 finished with value: 367.7 and parameters: {'hidden_dim': 256, 'inner_lr': 0.001970189881588617, 'meta_lr': 0.001712932848923796, 'thompson_alpha': 0.9581264894679202, 'thompson_beta': 1.9894245534786883, 'epsilon_decay': 0.9856451097217668, 'batch_size': 248, 'memory_size': 5400, 'gamma': 0.9167420379032988}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:30:45,108] Trial 61 finished with value: 166.8 and parameters: {'hidden_dim': 256, 'inner_lr': 0.00179897520757152, 'meta_lr': 0.0017354750660654725, 'thompson_alpha': 0.9385249174317554, 'thompson_beta': 1.924151212425428, 'epsilon_decay': 0.9925078209497413, 'batch_size': 248, 'memory_size': 5500, 'gamma': 0.9176862302212311}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:31:01,158] Trial 62 finished with value: 9.6 and parameters: {'hidden_dim': 256, 'inner_lr': 0.005277062215250572, 'meta_lr': 0.0010731904959740929, 'thompson_alpha': 0.687251521959374, 'thompson_beta': 1.9574590869464255, 'epsilon_decay': 0.9846912159525233, 'batch_size': 232, 'memory_size': 4700, 'gamma': 0.9077004610859365}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:31:28,252] Trial 63 finished with value: 45.0 and parameters: {'hidden_dim': 256, 'inner_lr': 0.002231494081880878, 'meta_lr': 0.006487208248736879, 'thompson_alpha': 1.0511838069631407, 'thompson_beta': 1.7266829735595919, 'epsilon_decay': 0.9868939389875219, 'batch_size': 248, 'memory_size': 6700, 'gamma': 0.9530434839698726}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:32:01,592] Trial 64 finished with value: 232.8 and parameters: {'hidden_dim': 224, 'inner_lr': 0.00024535945385499407, 'meta_lr': 0.0019706792416037074, 'thompson_alpha': 1.4392015943991796, 'thompson_beta': 0.527593936365709, 'epsilon_decay': 0.9886637327984136, 'batch_size': 216, 'memory_size': 3900, 'gamma': 0.9036817381039183}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:32:34,961] Trial 65 finished with value: 254.6 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0003707183245696457, 'meta_lr': 0.00149869932259705, 'thompson_alpha': 1.6956018929532979, 'thompson_beta': 1.5653190283943474, 'epsilon_decay': 0.9824101302939703, 'batch_size': 256, 'memory_size': 5400, 'gamma': 0.9111314810795381}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:33:05,071] Trial 66 finished with value: 246.1 and parameters: {'hidden_dim': 256, 'inner_lr': 0.0006086655785583086, 'meta_lr': 0.0007954888753981174, 'thompson_alpha': 1.290195988747218, 'thompson_beta': 1.8043807537163556, 'epsilon_decay': 0.9748833004224405, 'batch_size': 192, 'memory_size': 6400, 'gamma': 0.9638542145885308}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:33:19,361] Trial 67 finished with value: 10.4 and parameters: {'hidden_dim': 224, 'inner_lr': 3.824826642936153e-05, 'meta_lr': 2.3417886250593694e-05, 'thompson_alpha': 0.6352274032462878, 'thompson_beta': 1.4134406447435905, 'epsilon_decay': 0.9785244911135251, 'batch_size': 240, 'memory_size': 9700, 'gamma': 0.9354069199030265}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:33:58,031] Trial 68 finished with value: 207.0 and parameters: {'hidden_dim': 256, 'inner_lr': 0.00013223012139158323, 'meta_lr': 0.002856680595483596, 'thompson_alpha': 1.5156356146292946, 'thompson_beta': 1.9986618903263889, 'epsilon_decay': 0.99013386315721, 'batch_size': 208, 'memory_size': 5800, 'gamma': 0.9081653848380297}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:34:21,567] Trial 69 finished with value: 326.7 and parameters: {'hidden_dim': 64, 'inner_lr': 0.0013959221953958082, 'meta_lr': 0.002333282826382233, 'thompson_alpha': 0.18135275568489678, 'thompson_beta': 1.2395725219484945, 'epsilon_decay': 0.9853074466617682, 'batch_size': 136, 'memory_size': 9000, 'gamma': 0.9203774704350332}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:34:40,562] Trial 70 finished with value: 329.0 and parameters: {'hidden_dim': 192, 'inner_lr': 1.944171979959753e-05, 'meta_lr': 0.0005392670065206712, 'thompson_alpha': 0.7590827642834497, 'thompson_beta': 1.6645724340275534, 'epsilon_decay': 0.9810762413321057, 'batch_size': 160, 'memory_size': 8400, 'gamma': 0.916475130150159}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:34:59,979] Trial 71 finished with value: 199.0 and parameters: {'hidden_dim': 192, 'inner_lr': 1.6076215790606514e-05, 'meta_lr': 0.0006603249697808407, 'thompson_alpha': 0.9261626879294169, 'thompson_beta': 1.6258577111920112, 'epsilon_decay': 0.9809042763660671, 'batch_size': 160, 'memory_size': 8600, 'gamma': 0.9263589318948621}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:35:27,858] Trial 72 finished with value: 246.0 and parameters: {'hidden_dim': 224, 'inner_lr': 2.012061761544959e-05, 'meta_lr': 0.0009377465451541995, 'thompson_alpha': 0.773096909967026, 'thompson_beta': 1.8611103536447287, 'epsilon_decay': 0.9835713618017531, 'batch_size': 112, 'memory_size': 9800, 'gamma': 0.9163981473102925}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:35:44,166] Trial 73 finished with value: 45.7 and parameters: {'hidden_dim': 160, 'inner_lr': 6.076308827453675e-05, 'meta_lr': 0.00040803893945838825, 'thompson_alpha': 1.1423862931352975, 'thompson_beta': 1.4562679156786593, 'epsilon_decay': 0.9772717795602127, 'batch_size': 184, 'memory_size': 9400, 'gamma': 0.9314329021815426}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:36:04,707] Trial 74 finished with value: 216.4 and parameters: {'hidden_dim': 192, 'inner_lr': 4.321666229430566e-05, 'meta_lr': 0.0005747448733161906, 'thompson_alpha': 1.0107720715717954, 'thompson_beta': 1.6624524590598801, 'epsilon_decay': 0.9794400867899059, 'batch_size': 136, 'memory_size': 9200, 'gamma': 0.9145850214197038}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:36:32,865] Trial 75 finished with value: 392.5 and parameters: {'hidden_dim': 224, 'inner_lr': 2.9959389057255964e-05, 'meta_lr': 0.0013251274197024945, 'thompson_alpha': 0.8525217484557134, 'thompson_beta': 0.34331807583338214, 'epsilon_decay': 0.9879430536955458, 'batch_size': 96, 'memory_size': 4800, 'gamma': 0.9590277397576947}. Best is trial 18 with value: 427.6.\n",
            "[I 2024-12-06 09:37:06,150] Trial 76 finished with value: 205.7 and parameters: {'hidden_dim': 256, 'inner_lr': 2.572260088196232e-05, 'meta_lr': 0.001294834723387847, 'thompson_alpha': 0.4953432203407953, 'thompson_beta': 0.3280425191972148, 'epsilon_decay': 0.992726934458676, 'batch_size': 88, 'memory_size': 6300, 'gamma': 0.9577447593370816}. Best is trial 18 with value: 427.6.\n",
            "[W 2024-12-06 09:37:16,243] Trial 77 failed with parameters: {'hidden_dim': 256, 'inner_lr': 3.31241650322418e-05, 'meta_lr': 0.00026413734601698444, 'thompson_alpha': 0.7399946895688077, 'thompson_beta': 0.1726994695434201, 'epsilon_decay': 0.9820541532082638, 'batch_size': 104, 'memory_size': 4900, 'gamma': 0.9535425184230149} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-15-20944992d631>\", line 56, in objective\n",
            "    agent.meta_update()\n",
            "  File \"<ipython-input-9-f2653cb82324>\", line 133, in meta_update\n",
            "    states, actions, rewards, next_states, dones = self.prepare_batch(eval_batch)\n",
            "  File \"<ipython-input-9-f2653cb82324>\", line 91, in prepare_batch\n",
            "    actions = torch.LongTensor(actions).to(self.device)\n",
            "KeyboardInterrupt\n",
            "[W 2024-12-06 09:37:16,246] Trial 77 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-20944992d631>\u001b[0m in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Plot optimization history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-20944992d631>\u001b[0m in \u001b[0;36moptimize_hyperparameters\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptimize_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best trial:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-20944992d631>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Update target network periodically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f2653cb82324>\u001b[0m in \u001b[0;36mmeta_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# Evaluate on evaluation batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f2653cb82324>\u001b[0m in \u001b[0;36mprepare_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
        "    inner_lr = trial.suggest_loguniform(\"inner_lr\", 1e-5, 1e-1)\n",
        "    meta_lr = trial.suggest_loguniform(\"meta_lr\", 1e-5, 1e-2)\n",
        "    thompson_alpha = trial.suggest_uniform(\"thompson_alpha\", 0.1, 2.0)\n",
        "    thompson_beta = trial.suggest_uniform(\"thompson_beta\", 0.1, 2.0)\n",
        "    epsilon_decay = trial.suggest_uniform(\"epsilon_decay\", 0.95, 0.999)\n",
        "    batch_size = trial.suggest_int(\"batch_size\", 8, 256, step=8)\n",
        "    memory_size = trial.suggest_int(\"memory_size\", 1000, 10000, step=100)\n",
        "    gamma = trial.suggest_uniform(\"gamma\", 0.9, 0.999)\n",
        "\n",
        "    # Initialize environment and agent with sampled hyperparameters\n",
        "    env_name = \"CartPole-v1\"\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    agent = ThompsonMetaQLearning(\n",
        "        state_dim=state_dim,\n",
        "        action_dim=action_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        inner_lr=inner_lr,\n",
        "        meta_lr=meta_lr,\n",
        "        thompson_alpha=thompson_alpha,\n",
        "        thompson_beta=thompson_beta,\n",
        "        batch_size=batch_size,\n",
        "        #memory_size=memory_size,\n",
        "        gamma=gamma\n",
        "    )\n",
        "\n",
        "    episodes = 500\n",
        "    max_steps = 500\n",
        "    scores = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores.append(total_reward)\n",
        "        agent.meta_update()\n",
        "\n",
        "        # Update target network periodically\n",
        "        if episode % 10 == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        # Decay epsilon\n",
        "        agent.epsilon = max(0.1, agent.epsilon * epsilon_decay)\n",
        "\n",
        "    # Objective: maximize the mean reward over the last 10 episodes\n",
        "    return np.mean(scores[-10:])\n",
        "\n",
        "# Run Optuna study\n",
        "def optimize_hyperparameters():\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=100)\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(f\"  Value: {trial.value}\")\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "\n",
        "    return study\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optimize_hyperparameters()\n",
        "\n",
        "    # Plot optimization history\n",
        "    optuna.visualization.plot_optimization_history(study)\n",
        "    plt.title(\"Optimization History\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot parameter importance\n",
        "    optuna.visualization.plot_param_importances(study)\n",
        "    plt.title(\"Parameter Importance\")\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}